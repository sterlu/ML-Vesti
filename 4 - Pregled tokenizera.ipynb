{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Analiza BPE tokenizera\n",
    "\n",
    "U nastavku su opažanja primećena pri korišćenju BPE tokenizera\n",
    "baziranog na pomenutom setu podataka."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "bpe_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./naslovi-tokenizer.json\")\n",
    "simple_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"./naslovi-simple-tokenizer.json\")\n",
    "bpe_tokenizer.pad_token = \"<pad>\"\n",
    "simple_tokenizer.pad_token = \"<pad>\"\n",
    "\n",
    "def tokenizuj(tokenizer, tekst):\n",
    "    print(tokenizer.tokenize(tekst), tokenizer.encode(tekst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prvo što primećujemo je da BPE tokenizer drugačije enkoduje reč\n",
    "ako je na početku rečenice i ako je u sredini."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Danas'] [1852]\n",
      "['ĠDanas'] [6431]\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(bpe_tokenizer, 'Danas')\n",
    "tokenizuj(bpe_tokenizer, ' Danas')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Možemo da vidimo i da tokenizer isto tretira reči na početku\n",
    "rečenice kao sufiksne nastavke koje je sam kreirao. Ovo ponašanje\n",
    "je neočekivano ali se možda objašnjava postojanjem tokena za\n",
    "početak rečenice u trening setu."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Partiza', 'nove', 'Ġnove', 'Ġpri', 'nove'] [1904, 1923, 1409, 399, 1923]\n",
      "['nove', 'ĠPartizanove', 'Ġpri', 'nove'] [1923, 39971, 399, 1923]\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(bpe_tokenizer, 'Partizanove nove prinove')\n",
    "tokenizuj(bpe_tokenizer, 'nove Partizanove prinove')\n",
    "# obratiti pažnju na token 1923"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ilustrativno prikazujemo da klasičan tokenizer enkoduje ove\n",
    "reči isto nezavisno od lokacije, ali na primer reč \"prinove\"\n",
    "nema u rečniku."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Partizanove', 'prinove'] [20700, 3]\n",
      "['Partizanove', 'prinove'] [20700, 3]\n",
      "Partizanove <unk>\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(simple_tokenizer, 'Partizanove prinove')\n",
    "tokenizuj(simple_tokenizer, ' Partizanove prinove')\n",
    "print(simple_tokenizer.decode(simple_tokenizer.encode('Partizanove prinove')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Česte reči ipak imaju svoj token u vokabularu, iako se mogu sastaviti\n",
    "od drugih. Na primer, postoji \"Kraljevo\", ali i \"Kralje\" i \"-vo\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kralje', 'vo', 'ĠKraljevo', 'ĠKralje', 'vski'] [15284, 287, 32176, 5095, 3841]\n",
      "['ĠBeo', '-', 'grad', 'ĠBeograd', 'ĠBeogradu', 'ĠBeogradski'] [16632, 15, 3052, 1375, 986, 19432]\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(bpe_tokenizer, 'Kraljevo Kraljevo Kraljevski')\n",
    "tokenizuj(bpe_tokenizer, ' Beo-grad Beograd Beogradu Beogradski')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iako je možda očekivano da nađemo primere gde tokenizer odvaja\n",
    "osnovu reči i sufikse (na primer glagolsku osnovu i nastavke za\n",
    "vreme i rod), to je slučaj samo kada reč nije dovoljno česta da\n",
    "ima svoj token (npr \"videvši\")."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġvikao', 'Ġvikala', 'ĠviÄįe'] [16158, 28843, 24093]\n",
      "['ĠuÅ¾ivao', 'ĠuÅ¾ivaju', 'ĠuÅ¾iva'] [25471, 11842, 4046]\n",
      "['Ġvideo', 'Ġvideli', 'Ġvidela', 'Ġvide', 'vÅ¡i'] [1991, 2214, 4513, 954, 25014]\n",
      "['Ġpeva', 'Ġpevao', 'Ġpevala'] [1680, 11726, 11144]\n",
      "['Ġvozio', 'Ġvozi', 'Ġvozi', 'Äĩe', 'ĠvozaÄį', 'Ġvozili'] [7036, 1394, 1394, 320, 4134, 24893]\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(bpe_tokenizer, ' vikao vikala viče')\n",
    "tokenizuj(bpe_tokenizer, ' uživao uživaju uživa')\n",
    "tokenizuj(bpe_tokenizer, ' video videli videla videvši')\n",
    "tokenizuj(bpe_tokenizer, ' peva pevao pevala')\n",
    "tokenizuj(bpe_tokenizer, ' vozio vozi voziće vozač vozili')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vidimo da se složene reči koje nisu dovoljno česte da imaju\n",
    "svoj token sastavljaju od drugih tokena kao što bismo očekivali\n",
    "(npr. \"prekosutra\" iz \"preko\" i \"-sutra\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġnovo', 'na', 'stala', 'Ġnovo', 'gradnja', 'Ġi', 'Ġu', 'gradnja'] [1457, 263, 3829, 1457, 19600, 266, 273, 19600]\n",
      "['ĠnoÄĩ', 'ĠponoÄĩ', 'ĠsinoÄĩ', 'Ġpre', 'ksi', 'noÄĩ'] [2902, 15056, 6288, 351, 1165, 5087]\n",
      "['Ġsutra', 'Ġpreko', 'sutra'] [1648, 1459, 31974]\n",
      "['Ġputa', 'Ġpreko', 'puta', 'Ġtri', 'puta'] [1474, 1459, 8308, 917, 8308]\n"
     ]
    }
   ],
   "source": [
    "tokenizuj(bpe_tokenizer, ' novonastala novogradnja i ugradnja')\n",
    "tokenizuj(bpe_tokenizer, ' noć ponoć sinoć preksinoć')\n",
    "tokenizuj(bpe_tokenizer, ' sutra prekosutra')\n",
    "tokenizuj(bpe_tokenizer, ' puta prekoputa triputa')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}