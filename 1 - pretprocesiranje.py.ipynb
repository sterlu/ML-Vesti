{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_filename = \"novi_podaci/post_naslovi.input\"\n",
    "dataset_filename = input_filename + '.prepared'\n",
    "pretrained_model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import alati\n",
    "\n",
    "input_file = open(input_filename, \"r\")\n",
    "output_file = open(dataset_filename, \"w\")\n",
    "\n",
    "skip_filter = re.compile(r'(^|[^\\w])(sunčano|kiša|oblačno|toplo|stepena|stepen|kišom|hladno|hladnije|stepeni|obilne padavine|toplije|sneg|ubije|ubil|poginu|silova|prebil|umrl|umro|preminu|mrtvi|dečak|devojčic|dete)', flags=re.I | re.M)\n",
    "\n",
    "for line in input_file:\n",
    "    line = alati.konvertuj_cirilicu(line)\n",
    "\n",
    "    if skip_filter.search(line):\n",
    "        continue\n",
    "\n",
    "    line = line.replace('ž', 'ž').replace('š', 'š').replace('&quot;', '\"').replace('&amp;', '&')\n",
    "    line = re.sub(r'^([\"”])*', '', line)\n",
    "    line = re.sub(r'([\"”])*$', '', line)\n",
    "    line = line.replace('– ', ' ') \\\n",
    "        .replace('- ', ' ') \\\n",
    "        .replace(':', ' ') \\\n",
    "        .replace('​', ' ')\n",
    "\n",
    "    line = line.replace('(KURIR TV)', '(VIDEO)').replace('KURIR TV', '')\n",
    "    re.sub('za (kurir|blic)', 'za Fleš', line, flags=re.I)\n",
    "    re.sub('uz (kurir|blic)', 'uz Fleš', line, flags=re.I)\n",
    "    re.sub(' (kurir|blic) vam ', ' Fleš vam ', line, flags=re.I)\n",
    "    re.sub('KURIR HOROSKOP', 'HOROSKOP', line, flags=re.I)\n",
    "\n",
    "    output_file.write(\"<s>\" + line.strip() + \"</s>\" + '\\n')\n",
    "\n",
    "input_file.close()\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=False)\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    # normalizers.Strip(),\n",
    "    normalizers.NFKC(),\n",
    "])\n",
    "tokenizer.train(\n",
    "    files=[dataset_filename],\n",
    "    min_frequency=10,\n",
    "    vocab_size=25000,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "        \"<|endoftext|>\",\n",
    "    ],\n",
    ")\n",
    "tokenizer.save('naslovi-tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"naslovi-tokenizer.json\")\n",
    "tokenizer.pad_token = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-74504fb82070669d\n",
      "Reusing dataset text (/Users/sterlu/.cache/huggingface/datasets/text/default-74504fb82070669d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n",
      "Loading cached processed dataset at /Users/sterlu/.cache/huggingface/datasets/text/default-74504fb82070669d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-36002bf1956e168e.arrow\n",
      "Loading cached processed dataset at /Users/sterlu/.cache/huggingface/datasets/text/default-74504fb82070669d/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5/cache-41d9e73e1c8ec5f9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('text', data_files=dataset_filename)\n",
    "def prepare_for_trainer(example):\n",
    "  tokenized = tokenizer(example['text'], truncation=True, padding='max_length', max_length=20)\n",
    "  tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "  return tokenized\n",
    "tokenized_datasets = datasets.map(prepare_for_trainer, num_proc=2, remove_columns=[\"text\"])\n",
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-de76209308de1b76\n",
      "Reusing dataset text (/Users/sterlu/.cache/huggingface/datasets/text/default-de76209308de1b76/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'text': '<s>Voditeljka godinama ćuti o ljubavnom životu, a NJEN BIVŠI MUŽ JE KUM LUNE I MARKA  Nisam rekla DA na venčanju</s>'}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, eval = load_dataset('text', data_files=dataset_filename, split=['train[:10%]', 'train[10%:]'])\n",
    "train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /Users/sterlu/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.8.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /Users/sterlu/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.8.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /Users/sterlu/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, AutoConfig, AutoModelForCausalLM, TrainingArguments\n",
    "\n",
    "config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
    "# model.to('cuda:0')\n",
    "training_args = TrainingArguments(\n",
    "    \"test-clm\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy='epoch',\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    # data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    # eval_dataset=train_dataset,\n",
    "    # prediction_loss_only=True,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 706443\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 264918\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='264918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    21/264918 00:47 < 185:34:45, 0.40 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-f40c36cd72e9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msave_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001B[0m\n\u001B[1;32m   1267\u001B[0m                         \u001B[0mtr_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1268\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1269\u001B[0;31m                     \u001B[0mtr_loss\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1270\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrent_flos\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mfloat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloating_point_ops\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1271\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.9/site-packages/transformers/trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   1770\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdeepspeed\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1771\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1772\u001B[0;31m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1773\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1774\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    253\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    254\u001B[0m                 inputs=inputs)\n\u001B[0;32m--> 255\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    256\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    257\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/miniconda3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    145\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 147\u001B[0;31m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[1;32m    148\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./test-clm/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"distilgpt2\",\n",
      "  \"_num_labels\": 1,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.8.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file ./test-clm/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./test-clm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('./test-clm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###### Temperature: 0.2\n",
      "U BiH se ne zna da li će biti biti i to\n",
      "U BiH se ne može da se ne zna da li će se ne može biti sa njom,\n",
      "U BiH se ne znate, a onda se ne zna da se ne zna šta je sve\n",
      "Na korona virus pozitivno još uvekmo u Srbiji\n",
      "U BiH se ne zna da li će biti biti i dalje u Srbiji\n",
      "\n",
      "###### Temperature: 0.4\n",
      "U Srbiji 24 časa od korona virusa\n",
      "U BiH se na čelu kriminom gradu, a evo šta je sve to znači\n",
      "U BiH nije mogao da bude se vrati, a onda je ona i dalje nije mogao da je\n",
      "U BiH se na ulice u Srbiji se pitaju je u centru grada, a onda je i to\n",
      "Apel na Novom Beogradu  Ne želim da se ne može biti u Srbiji\n",
      "\n",
      "###### Temperature: 0.6000000000000001\n",
      "U Srpskoj i dalje u Srbiji, a evo šta kaže da je to NIJE OBRI\n",
      "Mali  Povezna bi da bude u bolnici u Beogradu, ali se da li će biti\n",
      "U Grčkoj još jedan dan u šoku, ali i u Srbiji ima samo spove Srbiji\n",
      "Za vreme u Prištini da se okrem u Srbiji\n",
      "U Srbiji 24 časa novozaraženihženih BiH\n",
      "\n",
      "###### Temperature: 0.8\n",
      "Ko je bio i u vezi sa dečkom (VIDEO)\n",
      "Vučić  Sakolje da bi i dalje na Kosovu, evo i koje se u Srbiji i\n",
      "U ovom gradu u Srbiji novozaraženih korona virusa u Srpskoj\n",
      "Ipo li se bori za rad, pa mu da ostane na respiratorima su bili\n",
      "Svi je samo da se ne treba da se vrati, a onda je usledio  Naj to je\n",
      "\n",
      "###### Temperature: 1.0\n",
      "U Beogradu još jedna osoba4 milijardeizolih u Blicu\n",
      "Zbog 13 od ponedeljka (36) posle toga što nije bilo iz Crne Gore! Evo zašto se\n",
      "Pogledajte kako se na Tviteru, pa je i dalje sam  I NE IZGLEDAJU I VAŠI\n",
      "U BiH 24 sata 24 Clanju noći pređu, a kada su se bavi o Crnu\n",
      "Evo šta sve se dešava sa sinom, a onda je sada to bio, zbog jednog ne vidi\n",
      "\n",
      "###### Temperature: 1.2\n",
      "Dokulo Srbije  Pril sam još još to uvekmo po pitanju! Sput bi\n",
      "Dao promete je na protestu na Kosovu, a ona je bio na ovaj odgovor je uradila\n",
      "Šta je sa sada, drugi ko su sve ko je prvi čovek, ali onda ju je na\n",
      "Netm  Prostrit je to na severu smrtnom kaalu\n",
      "Pogledajte samoizomm si da se i premo da čekati i najfet\n",
      "\n",
      "###### Temperature: 1.4000000000000001\n",
      "Da li sve češćeski na terenu u Novom Sadu LESKAD DA ME JE TUPELA\n",
      "Još dve i Abaarlj, bez struje  U BiH sam po Beogradu su ne prestaju\n",
      "Netanijahu nije znaomo da mu sve pre 20 sa decom! Do da postane do sada,\n",
      "Kako možete da su se nešto nešto tu je u poseti kojima neće tačno da ostane sve nije vi\n",
      "Kako sam da uđe! Kako ne bude po dom za to biti mi pandeju, a\n",
      "\n",
      "###### Temperature: 1.6\n",
      "Naslovna BEZ10 godina ima korona 19 časova Subotić kako ni i ovo kaže, nije bilo\n",
      "Pre deset više će oduševiti raditi u kući da je PREĐEĐERE DA SE SVEJE SA\n",
      "Sve više ne mogu da li ne sme po svetu od 100.000 radnih se suo da ne\n",
      "Za sve je imao se obratio na izbore do 8 evra, a evo ko mu priča ona za\n",
      "Ipočiranavanje, ali nije ni više da izgleda NIJE NIPEVAM KA\n",
      "\n",
      "###### Temperature: 1.8\n",
      "Neju se jali kada će je danas morati i o finansi NIM REVSEP\n",
      "Zbog pafmati  Radovi kako bi ovo da li ste čeka ni kao! Da\n",
      "U Banjaluci je prvi zbog diglanjelo fiilo za ulazak zarađit do sada\n",
      "Nestoošskađenocene ACWpdenom su u jednom samo više\n",
      "Majedili da nas ni do 17 godine ili da li NE pot nas u Srpskoj\n",
      "\n",
      "###### Temperature: 2.0\n",
      "Od sutra pozvaone sve ko sve više u dvorištu porodici ne\n",
      "Avapvaljni za za novi na ulici BiH neće u kojoj su REZEŽST\n",
      "Uhapšen muškarac zbog jednog do 5 godine je od korona virusa! I mora se nije očekivao ni\n",
      "Av je kao da je počela se i u centru zemlje gde ste, a evo šta su\n",
      "Prože, koliku će vas začuvi koji NIJE TRI MILIJARDE za predsednikane taksi i još\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokenizer.pad_token_id = '1'\n",
    "model.config.pad_token_id = 1\n",
    "for temp in np.linspace(0.2, 2, 10):\n",
    "    print()\n",
    "    print(\"###### Temperature: \" + str(temp))\n",
    "    for i in range(0, 5):\n",
    "        out = model.generate(do_sample=True, temperature=temp)[0]\n",
    "        print(tokenizer.decode(out, skip_special_tokens=True))\n",
    "# model.generate()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}